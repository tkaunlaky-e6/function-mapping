# Output Directory

Contains basis data and mapping results for each SQL function. Each function has a subdirectory named in lowercase.

## Data Flow

```
  How output files are created
  =============================

  Phase 1: Basis Agent                    Phase 2: LATS Pipeline
  (python -m dialect_mapper.basis.agent)  (python -m dialect_mapper.lats.pipeline)

  +---------------------------+           +---------------------------+
  | LLM agent executes        |           | Pipeline reads basis,     |
  | queries on PySpark,       |           | executes on e6data,       |
  | writes:                   |           | runs LATS for failures,   |
  |                           |           | writes:                   |
  | output/<func>/            |           | output/<func>/            |
  |   basis_table.csv    -----+---------->|   e6_coverage.csv         |
  |   basis_analysis.json ----+---------->|   e6_mapping.json         |
  +---------------------------+           +---------------------------+

  basis_table.csv -----> input to LATS pipeline
  basis_analysis.json -> domain context for LLM prompts
  e6_coverage.csv -----> per-query execution results
  e6_mapping.json -----> final translations + coverage matrix
```

## Directory Structure

```
output/
  abs/
    basis_table.csv         8 queries, 8 partitions, 5 codomain classes
    basis_analysis.json     Domain analysis with spanning proofs
    e6_coverage.csv         Per-query execution results
    e6_mapping.json         Translations, coverage matrix, minimal function set

  concat_ws/
    basis_table.csv         25 queries — variadic parameter handling
    basis_analysis.json     Complex domain: sep, args, arrays, types
    basis_analysis_new.json Updated analysis (pipeline reads this first)

  factorial/
    basis_table.csv         13 queries — negative, overflow, type casting
    basis_analysis.json     10 partitions, 4 codomain classes
    e6_coverage.csv         Shows FACTORIAL(negative) fails on e6data
    e6_mapping.json         LATS discovers CASE WHEN wrappers needed

  datediff/
    basis_table.csv         26 queries — date/timestamp combinations
    basis_analysis.json     Coupled parameters (endDate, startDate)
    e6_mapping.json         LATS discovers reversed parameter order

  web_sales.csv             TPC-DS sample table (loaded as recent_web_sales in PySpark)
```

## File Formats

### basis_table.csv

Generated by the basis synthesis agent (Phase 1). Input to the LATS pipeline.

**Columns**: `id, query, input_partition, actual_output, codomain_class, reasoning`

```
id,query,input_partition,actual_output,codomain_class,reasoning
1,SELECT ABS(NULL),P_NULL,NULL,O_NULL,NULL propagation
2,SELECT ABS(-42),P_NEGATIVE_INT,42,O_POSITIVE_INT,Negation
3,SELECT ABS(0),P_ZERO,0,O_ZERO,Identity at zero
4,SELECT ABS(99),P_POSITIVE_INT,99,O_POSITIVE_INT,Identity
5,SELECT ABS(-3.14),P_NEGATIVE_DECIMAL,3.14,O_POSITIVE_DECIMAL,Negation of decimal
```

**CSV quoting**: any field containing a comma must be double-quoted:
```
5,"SELECT CONCAT_WS(',', 'a', 'b', 'c')",P_MULTI_STRING,"a,b,c",O_JOINED,Comma join
```

### basis_analysis.json

Full mathematical domain analysis generated by the basis agent. Provides context to the LATS LLM.

```json
{
  "function": "ABS",
  "parameters": [
    {
      "name": "n",
      "partitions": [
        {
          "id": "P_NULL",
          "predicate": "n IS NULL",
          "representative": "NULL",
          "reasoning": "NULL propagation behavior"
        },
        {
          "id": "P_NEGATIVE_INT",
          "predicate": "n IN Z, n < 0",
          "representative": "-42",
          "reasoning": "Negation behavior"
        }
      ],
      "spanning_proof": {
        "covering": "P_NULL ∪ P_NEGATIVE_INT ∪ P_ZERO ∪ P_POSITIVE_INT ∪ ... = D",
        "disjoint": "Predicates are mutually exclusive"
      }
    }
  ],
  "output_analysis": {
    "range": "{NULL} ∪ {0} ∪ R+",
    "codomain_classes": [
      {"id": "O_NULL", "predicate": "output IS NULL"},
      {"id": "O_ZERO", "predicate": "output = 0"},
      {"id": "O_POSITIVE_INT", "predicate": "output IN Z, output > 0"},
      {"id": "O_POSITIVE_DECIMAL", "predicate": "output IN R\\Z, output > 0"},
      {"id": "O_LARGE_INT", "predicate": "output > 2^53"}
    ],
    "surjectivity": {
      "coverage_ratio": 1.0,
      "all_classes_covered": true
    }
  },
  "basis_statistics": {
    "total_queries": 8,
    "spanning_complete": true,
    "surjectivity_complete": true
  }
}
```

### e6_coverage.csv

Per-query execution results from the LATS pipeline. Shows which queries passed direct execution vs needed LATS translation.

**Columns** (13):
```
id, spark_query, input_partition, expected_output, actual_output,
codomain_class, direct_pass, direct_error, e6_function, e6_query,
lats_pass, lats_score, covered
```

```
1,SELECT ABS(NULL),P_NULL,NULL,NULL,O_NULL,True,,ABS,SELECT ABS(NULL),,,True
7,SELECT ABS(-9223372036854775808),P_MIN_BIGINT,9223372036854775808,,O_LARGE_INT,False,overflow,...
```

### e6_mapping.json

Final output: translations, coverage matrix, and minimal function set.

```json
{
  "spark_function": "FACTORIAL",
  "total_basis_queries": 13,
  "overall_coverage": 1.0,
  "minimal_function_set": ["FACTORIAL", "DIRECT"],
  "uncovered_partitions": [],
  "coverage_matrix": {
    "FACTORIAL": {
      "covered_queries": [1, 3, 4, 5, 6, 7, 8, 9, 10],
      "count": 9,
      "coverage": 0.69
    },
    "DIRECT": {
      "covered_queries": [2, 11, 12, 13],
      "count": 4,
      "coverage": 0.31
    }
  },
  "translations": [
    {
      "id": 1,
      "spark_query": "SELECT FACTORIAL(NULL)",
      "input_partition": "P_NULL",
      "e6_query": "SELECT FACTORIAL(NULL)",
      "e6_function": "FACTORIAL",
      "covered": true,
      "reasoning": "NULL propagation — both dialects return NULL"
    },
    {
      "id": 2,
      "spark_query": "SELECT FACTORIAL(-5)",
      "input_partition": "P_NEGATIVE",
      "e6_query": "SELECT CASE WHEN -5 < 0 THEN NULL ELSE FACTORIAL(-5) END",
      "e6_function": "DIRECT",
      "covered": true,
      "reasoning": "e6data throws on negative input; CASE wrapper returns NULL like Spark"
    }
  ]
}
```

## Function-Specific Notes

### ABS (8 queries)

- 8 partitions: NULL, negative int, zero, positive int, negative decimal, positive decimal, min bigint, string cast
- 5 codomain classes: O_NULL, O_ZERO, O_POSITIVE_INT, O_POSITIVE_DECIMAL, O_LARGE_INT
- e6data difference: ABS(NULL) returns 0 instead of NULL. LATS wraps with CASE WHEN.
- P_MIN_BIGINT (-9223372036854775808) causes overflow on e6data.

### CONCAT_WS (25 queries)

- Variadic function: separator + variable number of arguments + arrays
- Partitions cover: NULL separator, no args, all NULL args, single survivor, multi-string, null skip, empty strings, int/decimal/boolean/date/timestamp args, array handling, nested functions, empty separator, multi-char separator
- Tests both scalar and array argument forms

### FACTORIAL (13 queries)

- 10 partitions: NULL, negative, zero, one, small positive, large positive (15, 20), overflow (21, 100), string cast, decimal
- Key discovery: Spark returns NULL for negative inputs and n>20. e6data throws `IllegalArgumentException`.
- LATS solution: `CASE WHEN n < 0 OR n > 20 THEN NULL ELSE FACTORIAL(n) END`

### DATEDIFF (26 queries)

- Coupled parameters (endDate, startDate) with DATE, TIMESTAMP, STRING types
- Tests: same day, positive/negative differences, NULL propagation, leap years, year/month boundaries, extreme date ranges, timestamps with time components
- Key discovery: e6data has reversed parameter order. `DATEDIFF(a,b) = b-a` vs Spark's `a-b`.
- LATS discovers this from output mismatches (-9 vs 9) and swaps parameter order.